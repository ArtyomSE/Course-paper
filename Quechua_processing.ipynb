{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgngl-oqniqe"
      },
      "source": [
        "# Получение словаря языка Кечуа"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at-tPHZLj52g"
      },
      "source": [
        "### 1. Получение названий статей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoWYau8o77Vi"
      },
      "outputs": [],
      "source": [
        "import urllib.request as url\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mrAE0q_R5mD"
      },
      "outputs": [],
      "source": [
        "def encode(string, code_dict):\n",
        "    pattern = re.compile(\"|\".join(map(re.escape, code_dict.keys())))\n",
        "    return pattern.sub(lambda match: code_dict[match.group(0)], string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKn7Ae_ZWRqK"
      },
      "outputs": [],
      "source": [
        "f = open('drive/MyDrive/Quechua/links.txt', 'r')\n",
        "links = f.read()\n",
        "f.close()\n",
        "\n",
        "qu_wiki_url = 'https://qu.wikipedia.org/w/index.php?title=Sapaq:TukuyPanqakuna&from='\n",
        "links = encode(links, {qu_wiki_url: '', '\\n': ' '})\n",
        "\n",
        "f = open('drive/MyDrive/Quechua/sections.txt', 'w')\n",
        "f.write(links)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY1-UctQ7x3S",
        "outputId": "59119f72-880e-476e-f7f6-9060c4bbcbf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество найденных статей: 41566\n"
          ]
        }
      ],
      "source": [
        "encoding_dict = {'ñ': '%C3%B1', 'ó': '%C3%B3', 'ø': '%C3%B8', 'á': '%C3%A1',\n",
        "                 'é': '%C3%A9', 'í': '%C3%AD', 'ž': '%C5%BE', \"\\'\": '%27',\n",
        "                 '(': '%28', ')': '%29', ',': '%2C', ':': '%3A'}\n",
        "articles_names = ''\n",
        "\n",
        "f = open('drive/MyDrive/Quechua/sections.txt', 'r')\n",
        "sections = encode(f.read(), encoding_dict).split(' ')\n",
        "f.close()\n",
        "\n",
        "for section in sections:\n",
        "    f = url.urlopen(qu_wiki_url + section)\n",
        "    articles = f.read()\n",
        "    f.close()\n",
        "\n",
        "    articles = BeautifulSoup(articles).get_text()\n",
        "    articles = encode(articles, {**{'\\n': '~', ' ': '_'}, **encoding_dict})\n",
        "    articles = articles.split('~')[273:-40]\n",
        "    articles[0] = section\n",
        "    articles_names += ' '.join(articles)\n",
        "\n",
        "    time.sleep(0.5)\n",
        "\n",
        "f = open('drive/MyDrive/Quechua/articles_names.txt', 'w')\n",
        "f.write(articles_names)\n",
        "f.close()\n",
        "\n",
        "articles_names = articles_names.split(' ')\n",
        "print(f'Количество найденных статей: {len(articles_names)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCUNk4bcuseu"
      },
      "source": [
        "### 2. Получение статей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv5hsrsxqV7z"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install pycryptodome==3.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QBHHT6UnbBH"
      },
      "outputs": [],
      "source": [
        "import urllib.request as url\n",
        "from urllib.error import HTTPError\n",
        "from bs4 import BeautifulSoup\n",
        "import PyPDF2 as pdf\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP1EM9Gbq0MG"
      },
      "outputs": [],
      "source": [
        "def remove_substr(s, start, end):\n",
        "    i = s.find(start)\n",
        "    j = s.find(end, i) + len(end)\n",
        "    s = s[:i] + s[j:]\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KdmwO0Luw2i",
        "outputId": "7864b79c-0d96-4cb1-9af3-f6ec50527d53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество доступных статей: 39981\n"
          ]
        }
      ],
      "source": [
        "qu_wiki_url = 'https://qu.wikipedia.org/wiki/'\n",
        "drive_texts_url = 'drive/MyDrive/Quechua/Texts/'\n",
        "\n",
        "f = open('drive/MyDrive/Quechua/articles_names.txt', 'r')\n",
        "articles_names = f.read().split(' ')\n",
        "f.close()\n",
        "\n",
        "remove_labels = ((' ', 'Wikipediamanta'), ('Katiguriya', 'contenido'),\n",
        "                 ('\\\"https', '(Wikipedia, Qhichwa / Quechua)'))\n",
        "\n",
        "for article_name in articles_names:\n",
        "    try:\n",
        "        f = url.urlopen(qu_wiki_url + article_name)\n",
        "    except (UnicodeEncodeError, HTTPError, FileNotFoundError):\n",
        "        continue\n",
        "\n",
        "    text = f.read()\n",
        "    text = BeautifulSoup(text).get_text()\n",
        "\n",
        "    for start, end in remove_labels:\n",
        "        text = remove_substr(text, start, end)\n",
        "\n",
        "    try:\n",
        "        f = open(drive_texts_url + article_name + '.txt', 'w')\n",
        "    except FileNotFoundError:\n",
        "        article_name = article_name.replace('/', '_')\n",
        "        f = open(drive_texts_url + article_name + '.txt', 'w')\n",
        "    f.write(text)\n",
        "    f.close()\n",
        "\n",
        "articles_names = os.listdir(drive_texts_url[:-1])\n",
        "print(f'Количество доступных статей: {len(articles_names)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsHfQXaJZmh9"
      },
      "source": [
        "### 3. Добавление pdf-файлов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqi5Rttf7l4i"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install pycryptodome==3.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnBxtu4k6pa7"
      },
      "outputs": [],
      "source": [
        "import PyPDF2 as pdf\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzEdSSm_qJSp",
        "outputId": "675cd2d0-5e27-40d5-d470-8f8551927261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"dialnet\" has 15 pages\n",
            "\"dictionary\" has 1443 pages\n",
            "\"evangelioc\" has 402 pages\n",
            "\"manual\" has 133 pages\n",
            "\"santa_clara\" has 76 pages\n",
            "\n",
            "Количество доступных текстов: 39986\n"
          ]
        }
      ],
      "source": [
        "pdf_names = ('dialnet', 'dictionary', 'evangelioc', 'manual', 'santa_clara')\n",
        "\n",
        "drive_texts_url = 'drive/MyDrive/Quechua/Texts/'\n",
        "\n",
        "for pdf_name in pdf_names:\n",
        "    pdf_file = open(f'drive/MyDrive/Quechua/PDF/{pdf_name}.pdf', 'rb')\n",
        "    pdf_reader = pdf.PdfReader(pdf_file)\n",
        "\n",
        "    pages_num = len(pdf_reader.pages)\n",
        "    print(f'\"{pdf_name}\" has {pages_num} pages')\n",
        "\n",
        "    txt_file = open(drive_texts_url + pdf_name + '.txt', 'a')\n",
        "    for page_num in range(pages_num):\n",
        "        text = pdf_reader.pages[page_num].extract_text()\n",
        "        txt_file.write(text)\n",
        "    txt_file.close()\n",
        "\n",
        "    pdf_file.close()\n",
        "\n",
        "texts_names = os.listdir(drive_texts_url[:-1])\n",
        "print(f'\\nКоличество доступных текстов: {len(texts_names)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9ZCuV_418MF"
      },
      "source": [
        "### 4. Очистка и лемматизация текстов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di-1h56n_6J3"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c37nPWLh2AdF"
      },
      "outputs": [],
      "source": [
        "def replace_symbols(symbols, replacement, text):\n",
        "    for symbol in symbols:\n",
        "        text = text.replace(symbol, replacement)\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    # Lowering case:\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removing punctuation and other extra symbols:\n",
        "    text = replace_symbols(',.[]{}()=≈>≥<≤+‡-±−*&^%$#@¡\"!~;:ː§/\\|¿?«»•·ºª↑↓←→≠Ø½∞', '', text)\n",
        "    text = text.replace('\\xa0', '')\n",
        "    text = text.replace('www', '')Г\n",
        "\n",
        "    # Replacing newline symbols with spaces:\n",
        "    text = text.replace('\\n', ' ')\n",
        "    for _ in range(5):\n",
        "        text = text.replace('  ', ' ')\n",
        "    if text[0] == ' ':\n",
        "        text = text[1:]\n",
        "    if text[-1] == ' ':\n",
        "        text = text[:-1]\n",
        "\n",
        "    # Replacing all numbers with masks:\n",
        "    text = replace_symbols('0123456789', 'NUM', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def replace_morphemes(text, morphemes, marker, first_layer=False):\n",
        "    if first_layer:\n",
        "        for morpheme in morphemes:\n",
        "            text = text.replace(morpheme + ' ', marker + ' ')\n",
        "    else:\n",
        "        for morpheme in morphemes:\n",
        "            text = text.replace(morpheme + marker, marker)\n",
        "    return text\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    ###  Numerals  ###\n",
        "    numeral_token = 'NUM'\n",
        "\n",
        "    tens = ('chunka', 'pachak', 'waranqa', 'hunu') # 10, 100, 1000, 1000000\n",
        "    for ten in tens:\n",
        "        text = text.replace(ten + ' ', numeral_token + ' ')\n",
        "\n",
        "    figures = ('chusaq', 'huk', 'iskay', 'kimsa', 'tawa', 'pichqa', 'suqta', 'qanchis', 'pusaq', 'isqun') # 0-9\n",
        "    for figure in figures:\n",
        "        text = text.replace(' ' + figure + ' ', ' ' + numeral_token + ' ')\n",
        "        text = text.replace(figure + numeral_token, numeral_token)\n",
        "\n",
        "    quantitative_suffixes = ('niyuq', 'yuq')\n",
        "    for quantitative_suffix in quantitative_suffixes:\n",
        "        for figure in figures:\n",
        "            text = text.replace(figure + quantitative_suffix, numeral_token)\n",
        "\n",
        "    ordinal_suffixes = ('ñaqin', 'ñiqin', 'kaq')\n",
        "    for  ordinal_suffix in ordinal_suffixes:\n",
        "        text = text.replace(' ' +  ordinal_suffix + ' ', ' ')\n",
        "\n",
        "    ###   Verbs   ###\n",
        "    infinitive_ending = 'y'\n",
        "    marker = '~'\n",
        "\n",
        "    # Interrogative to affirmative:\n",
        "    interrogative_suffix = 'chu'\n",
        "    text = text.replace(interrogative_suffix + ' ', ' ')\n",
        "\n",
        "    # Conjunctive:\n",
        "    simple_endings = ('yman', 'waq', 'nkiman', 'sunman', 'nman', 'sunchikman', 'chwan',\n",
        "                      'nchikman', 'ykuman', 'waqchik', 'nkichikman', 'nkuman')\n",
        "    complex_endings = ('yki', 'ykichik', 'ykiku', 'wanki', 'wankiku', # 'wan', <- not to mix\n",
        "                       'wanchik', 'wanku', 'sunki', 'sunkichik', 'sunkiku')\n",
        "    text = replace_morphemes(text, simple_endings + complex_endings, marker, True)\n",
        "\n",
        "    ### --------- ###\n",
        "\n",
        "    ###   Nouns   ###\n",
        "    # All cases to nominative:\n",
        "    cases_endings = ('p', 'pa', 'paq', 'manta', 'wan', 'ta', 'pi', 'kama', 'rayku', 'man', 'hina', 'pura')\n",
        "    text = replace_morphemes(text, cases_endings, '', True)\n",
        "    # Plural to singular:\n",
        "    text = text.replace('kuna ', ' ')\n",
        "\n",
        "    ### --------- ###\n",
        "\n",
        "    # Indicative:\n",
        "    future_endings = ('saq', 'nki', 'nqa', 'sun', 'sunchik', 'saqku', 'nkichik', 'nqaku')\n",
        "    present_endings = ('ni', 'nki', 'n', 'nchik', 'yku', 'nkichik', 'nku')\n",
        "    text = replace_morphemes(text, future_endings + present_endings, marker, True)\n",
        "\n",
        "    past_narrative_continuous_suffixes = ('rqa', 'ra', 'sqa', 'chka')\n",
        "    text = replace_morphemes(text, past_narrative_continuous_suffixes, marker)\n",
        "\n",
        "    complex_suffix = 'q'\n",
        "    text = text.replace(complex_suffix + ' ka' + marker + ' ', marker + ' ')\n",
        "    text = text.replace(complex_suffix + ' ', marker + ' ')\n",
        "    text = text.replace(complex_suffix + 'ku ', marker + ' ')\n",
        "\n",
        "    # Imperative:\n",
        "    ending = 'ychik'\n",
        "    text = text.replace(ending + ' ', marker)\n",
        "\n",
        "    # Verbs to infinitive:\n",
        "    text = text.replace(marker, infinitive_ending)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gldkqnN2Atc"
      },
      "outputs": [],
      "source": [
        "drive_texts_url = 'drive/MyDrive/Quechua/Texts/'\n",
        "drive_processed_texts_url = 'drive/MyDrive/Quechua/Processed_Texts/'\n",
        "\n",
        "texts_names = os.listdir(drive_texts_url[:-1])\n",
        "texts_names.sort()\n",
        "\n",
        "for text_name in texts_names:\n",
        "    f = open(drive_texts_url + text_name, 'r')\n",
        "    text = f.read()\n",
        "    f.close()\n",
        "\n",
        "    text = clean_text(text)\n",
        "    text = lemmatize_text(text)\n",
        "\n",
        "    f = open(drive_processed_texts_url + text_name, 'w')\n",
        "    f.write(text)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypSeHHnw5WbC"
      },
      "source": [
        "### 5. Сборка корпуса Кечуа"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vam4G_975y2D"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6vOm-BCIg5e",
        "outputId": "cff88a76-fb27-4ba3-c471-0c2517fadb2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "В состав корпуса языка Кечуа вошло 39986 текстов\n"
          ]
        }
      ],
      "source": [
        "drive_processed_texts_url = 'drive/MyDrive/Quechua/Processed_Texts/'\n",
        "\n",
        "texts_names = os.listdir(drive_processed_texts_url[:-1])\n",
        "texts_names.sort()\n",
        "\n",
        "f_corpus = open('' + 'drive/MyDrive/Quechua/quechua_corpus.txt', 'w')\n",
        "\n",
        "for text_name in texts_names:\n",
        "    f_text = open(drive_processed_texts_url + text_name, 'r')\n",
        "    text = f_text.read()\n",
        "    f_corpus.write(text + '\\n')\n",
        "    f_text.close()\n",
        "\n",
        "f_corpus.close()\n",
        "print(f'В состав корпуса языка Кечуа вошло {len(texts_names)} текстов')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ_4m4si-WMJ"
      },
      "source": [
        "### 6. Создание TF_IDF матрицы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GEH4CfYVC7AJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_xH_RKAh-Wfc"
      },
      "outputs": [],
      "source": [
        "def create_tf_idf_matrix(corpus_path, min_df=1, use_idf=True):\n",
        "    '''\n",
        "    corpus_path - path to a corpus, where one line - one text\n",
        "\n",
        "    min_df - the minimum times (or fraction of the texts) a word must occur in the corpus\n",
        "\n",
        "    use_idf - flag of the usage of idf\n",
        "    '''\n",
        "    with open(corpus_path, 'r') as corpus_file:\n",
        "        vectorizer = TfidfVectorizer(analyzer='word', min_df=min_df, use_idf=use_idf)\n",
        "        data_vectorized = vectorizer.fit_transform(corpus_file)\n",
        "    return data_vectorized, vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrAcyAm7Er6f",
        "outputId": "a3123e3d-1dd4-4df0-d577-cfd659f9192c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размерность матрицы W: 39986 x 325662 (количество текстов x количество слов)\n",
            "\n",
            "Некоторые слова из полученного списка слов: ['abrirle' 'abrirme' 'abrirse' 'abrirá' 'abrirás']\n"
          ]
        }
      ],
      "source": [
        "drive_quechua_url = 'drive/MyDrive/Quechua/'\n",
        "\n",
        "W, words_list = create_tf_idf_matrix('drive/MyDrive/Quechua/quechua_corpus.txt')\n",
        "\n",
        "# Saving the TF-IDF matrix:\n",
        "f = open(drive_quechua_url + 'Matrices/' + 'tf_idf.npy', 'wb')\n",
        "np.save(f, W)\n",
        "f.close()\n",
        "\n",
        "# Saving the words list:\n",
        "f = open(drive_quechua_url + 'quechua_words_list.txt', 'w')\n",
        "f.write(' '.join(words_list))\n",
        "f.close()\n",
        "\n",
        "print(f'Размерность матрицы W: {W.shape[0]} x {W.shape[1]} (количество текстов x количество слов)\\n')\n",
        "print(f'Некоторые слова из полученного списка слов: {words_list[1000:1005]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhCaTzwzC0ME"
      },
      "source": [
        "### 7. SVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5tFS_0emC0U-"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse.linalg import svds\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "akeOVF6aC6pY"
      },
      "outputs": [],
      "source": [
        "def apply_svd(W, k, output_folder):\n",
        "    '''\n",
        "    W - TF-IDF matrix\n",
        "    k - the rank of the SVD (must be less than any dimension of W)\n",
        "    '''\n",
        "    # Apply the SVD function\n",
        "    u, sigma, vt = svds(W, k)\n",
        "\n",
        "    # Sorting singular values in descending order as function doesn't garantee it\n",
        "    descending_order_of_inds = np.flip(np.argsort(sigma))\n",
        "    u = u[:,descending_order_of_inds]\n",
        "    vt = vt[descending_order_of_inds]\n",
        "    sigma = sigma[descending_order_of_inds]\n",
        "\n",
        "    # Checking that sizes are correct\n",
        "    assert sigma.shape == (k,)\n",
        "    assert vt.shape == (k, W.shape[1])\n",
        "    assert u.shape == (W.shape[0], k)\n",
        "\n",
        "    # Save the matrixes in folder (just in case)\n",
        "    matrices_names = ('_sigma_vt.npy', '_sigma.npy', '_u.npy', '_vt.npy')\n",
        "    matrices = (np.dot(np.diag(sigma), vt).T, sigma, u, vt)\n",
        "\n",
        "    for matrix_name, matrix in zip(matrices_names, matrices):\n",
        "        with open(output_folder + str(k) + matrix_name, 'wb') as f:\n",
        "            np.save(f, matrix)\n",
        "\n",
        "    return np.dot(np.diag(sigma), vt).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbNodQVLLbFg",
        "outputId": "abc47156-26b1-4bf7-ce5f-e827b55ecc67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размерность матрицы vv: 325662 x 2 (количество слов x ранг k)\n"
          ]
        }
      ],
      "source": [
        "drive_matrices_url = 'drive/MyDrive/Quechua/Matrices/'\n",
        "\n",
        "vv = apply_svd(W, 2, drive_matrices_url)\n",
        "print(f'Размерность матрицы vv: {vv.shape[0]} x {vv.shape[1]} (количество слов x ранг k)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaDYWVLST9-N"
      },
      "source": [
        "### 8. Создание словаря"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qYk_7pbFT-Hi"
      },
      "outputs": [],
      "source": [
        "def create_dictionary(words_list, vv, output_file):\n",
        "  dictionary = {}\n",
        "  for word, vector in zip(words_list, vv):\n",
        "    dictionary[word] = vector\n",
        "  np.save(output_file, dictionary)\n",
        "  return dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6rMnlRW2UcQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d294e7d1-d683-4665-c219-3b432bb8be67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absentismo: [-1.83713903e-07 -7.26586961e-08]\n",
            "absey: [-6.12379677e-08 -2.42195654e-08]\n",
            "absi: [-9.93152074e-06 -7.91817497e-06]\n",
            "absida: [-0.00110513 -0.00109232]\n",
            "absidal: [-1.83713903e-07 -7.26586961e-08]\n",
            "absidatadrew: [-0.00036838 -0.00036411]\n",
            "absidia: [-0.00071713 -0.0005388 ]\n",
            "absinthe: [-0.00015453 -0.00013165]\n",
            "absinthum: [-3.39388756e-04 -9.79294703e-05]\n",
            "absintio: [-6.12379677e-08 -2.42195654e-08]\n"
          ]
        }
      ],
      "source": [
        "drive_quechua_url = 'drive/MyDrive/Quechua/'\n",
        "\n",
        "dictionary = create_dictionary(words_list[71:], vv[71:, :], drive_quechua_url + 'quechua_dictionary.npy')\n",
        "\n",
        "example_keys = list(dictionary.keys())[1000:1010]\n",
        "for example_key in example_keys:\n",
        "    print(f'{example_key}: {dictionary[example_key]}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}